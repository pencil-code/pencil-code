#!/bin/sh
# CVS: $Id$
#                       pc_start
#                      ----------
#   Run src/start.x (initialising f for src/run.x).
#   Start parameters are set in start.in.
#
# Run this script with csh:
#PBS -S /bin/sh
#$ -S /bin/sh
#@$-s /bin/sh
#
# Join stderr and stout:
#$ -j y -o run.log
#@$-eo
#
# Work in submit directory (SGE):
#$ -cwd
# Work in submit directory (PBS):
if [ -n "$PBS_O_WORKDIR" ]; then
  cd $PBS_O_WORKDIR
fi

# Work in submit directory (SUPER-UX's nqs):
if [ -n "$QSUB_WORKDIR" ]; then
  cd $QSUB_WORKDIR
fi

# Common setup for start.csh, run.csh, start_run.csh:
# Determine whether this is MPI, how many CPUS etc.
# This implicitly sources pc_functions.sh too.
debug=yes
. pc_config.sh

# Prevent code from running twice (and removing files by accident)
check_not_locked
# If local disc is used, write name into $datadir/directory_snap.
# This will be read by the code, if the file exists.
# Remove file, if not needed, to avoid confusion.
prepare_scratch_disk
#  If we don't have a data subdirectory: stop here (it is too easy to
#  continue with an NFS directory until you fill everything up).
check_datadir
# If the file NOERASE exists, the old directories are not erased
#   (start.x also knows then that var.dat is not created)
prepare_datadir
# If local disk is used, copy executable to $SCRATCH_DIR of master node
distribute_binary src/start.x

# Run start.x timestamping and timing appropriately
pencil_code_start

# If local disk is used, copy var.dat back to the data directory
final_copy_snapshots
tidy_scratch_disk

exit $start_status              # propagate status of mpirun

# cut & paste for job submission on the mhd machine
# bsub -n  4 -q 4cpu12h mpijob dmpirun src/start.x
# bsub -n  8 -q 8cpu12h mpijob dmpirun src/start.x
# bsub -n 16 -q 16cpu8h mpijob dmpirun src/start.x

# cut & paste for job submission for PBS
# qsub -l ncpus=64,mem=32gb,walltime=1:00:00 -W group_list=UK07001 -q UK07001 start.csh
# qsub -l nodes=4:ppn=1,mem=500mb,cput=24:00:00 -q p-long start.csh
# qsub -l ncpus=4,mem=1gb,cput=100:00:00 -q parallel start.csh
# qsub -l nodes=128,mem=64gb,walltime=1:00:00 -q workq start.csh
